{"metadata":{"colab":{"name":"summarization.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# wrap the output in colab cells\nfrom IPython.display import HTML, display\n\ndef set_css():\n  display(HTML('''\n  <style>\n    pre {\n        white-space: pre-wrap;\n    }\n  </style>\n  '''))\nget_ipython().events.register('pre_run_cell', set_css)\n\n# Install Transformers\n\n# install transformers with sentencepiece\n!pip install transformers[sentencepiece]\n\nimport pandas as pd\n\n# Specify the path to your uploaded file in Kaggle\nfile_path = \"/kaggle/input/education/education.txt\"\n\n# Read the file using pandas or other methods depending on the file type\ntry:\n    with open(file_path, \"r\") as file:\n        FileContent = file.read().strip()\n        # Now you can process the file_content as needed\nexcept FileNotFoundError:\n    print(f\"File not found at {file_path}\")\n\n\n# display file content\nFileContent \n\n# Read input file from Google Drive\n\n# total characters in the file\nlen(FileContent) \n\n# Load the Model and Tokenizer\n\n# import and initialize the tokenizer and model from the checkpoint\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ncheckpoint = \"sshleifer/distilbart-cnn-12-6\"\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n\n# Some model statistics\n\n# max tokens including the special tokens\ntokenizer.model_max_length \n\n# max tokens excluding the special tokens\ntokenizer.max_len_single_sentence \n\n# number of special tokens\ntokenizer.num_special_tokens_to_add() \n\n# Convert file content to sentences\n\n# extract the sentences from the document\nimport nltk\nnltk.download('punkt')\nsentences = nltk.tokenize.sent_tokenize(FileContent)\n\n# find the max tokens in the longest sentence\nmax([len(tokenizer.tokenize(sentence)) for sentence in sentences])\n\n# Create the chunks\n\n# initialize\nlength = 0\nchunk = \"\"\nchunks = []\ncount = -1\nfor sentence in sentences:\n  count += 1\n  combined_length = len(tokenizer.tokenize(sentence)) + length # add the no. of sentence tokens to the length counter\n\n  if combined_length  <= tokenizer.max_len_single_sentence: # if it doesn't exceed\n    chunk += sentence + \" \" # add the sentence to the chunk\n    length = combined_length # update the length counter\n\n    # if it is the last sentence\n    if count == len(sentences) - 1:\n      chunks.append(chunk.strip()) # save the chunk\n    \n  else: \n    chunks.append(chunk.strip()) # save the chunk\n    \n    # reset \n    length = 0 \n    chunk = \"\"\n\n    # take care of the overflow sentence\n    chunk += sentence + \" \"\n    length = len(tokenizer.tokenize(sentence))\nlen(chunks)\n\n# Some checks\n\n[len(tokenizer.tokenize(c)) for c in chunks]\n\n[len(tokenizer(c).input_ids) for c in chunks]\n\n## With special tokens added\n\nsum([len(tokenizer(c).input_ids) for c in chunks])\n\nlen(tokenizer(FileContent).input_ids)\n\n## Without special tokens added\n\nsum([len(tokenizer.tokenize(c)) for c in chunks])\n\nlen(tokenizer.tokenize(FileContent))\n\n# Get the inputs\n\n# inputs to the model\ninputs = [tokenizer(chunk, return_tensors=\"pt\") for chunk in chunks]\n\n# Output","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"z3HW7x111o9w","outputId":"04c13547-941a-48ae-9f7d-ea740779ebf3","execution":{"iopub.status.busy":"2023-10-08T08:51:53.183792Z","iopub.execute_input":"2023-10-08T08:51:53.184256Z","iopub.status.idle":"2023-10-08T08:52:09.747766Z","shell.execute_reply.started":"2023-10-08T08:51:53.184206Z","shell.execute_reply":"2023-10-08T08:52:09.746522Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n  <style>\n    pre {\n        white-space: pre-wrap;\n    }\n  </style>\n  "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n  <style>\n    pre {\n        white-space: pre-wrap;\n    }\n  </style>\n  "},"metadata":{}},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: transformers[sentencepiece] in /opt/conda/lib/python3.10/site-packages (4.33.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.3.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (4.66.1)\nRequirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.1.99)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (3.20.3)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers[sentencepiece]) (2023.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers[sentencepiece]) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers[sentencepiece]) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[sentencepiece]) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[sentencepiece]) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[sentencepiece]) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[sentencepiece]) (2023.7.22)\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"for input in inputs:\n  output = model.generate(**input)\n  print(tokenizer.decode(*output, skip_special_tokens=True))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"z3HW7x111o9w","outputId":"04c13547-941a-48ae-9f7d-ea740779ebf3","execution":{"iopub.status.busy":"2023-10-08T08:52:15.324487Z","iopub.execute_input":"2023-10-08T08:52:15.325026Z","iopub.status.idle":"2023-10-08T08:52:25.534316Z","shell.execute_reply.started":"2023-10-08T08:52:15.324987Z","shell.execute_reply":"2023-10-08T08:52:25.533145Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n  <style>\n    pre {\n        white-space: pre-wrap;\n    }\n  </style>\n  "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n  <style>\n    pre {\n        white-space: pre-wrap;\n    }\n  </style>\n  "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n  <style>\n    pre {\n        white-space: pre-wrap;\n    }\n  </style>\n  "},"metadata":{}},{"name":"stdout","text":" The Indian college education system is one of the largest and most diverse in the world. It plays a crucial role in shaping the future of millions of students and is a significant contributor to the country's economic and social development. The education system encompasses a wide range of institutions, courses, and approaches to education.\n","output_type":"stream"}]}]}